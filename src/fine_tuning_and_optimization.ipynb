{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXLox-P-Md6K"
   },
   "outputs": [],
   "source": [
    "!pip install kagglehub==0.3.5 speechbrain==1.0.2 torchaudio==2.5.1 soundfile==0.12.1 torch==2.5.1 tqdm==4.67.1 scikit-learn==1.6.0 transformers==4.47.1 datasets==3.2.0 jiwer==3.0.5 hugginface_hub==0.27.1 optuna optimum[onnxruntime] safetensors logger"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vocab Generator"
   ],
   "metadata": {
    "id": "7nt944GC_ryR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bl-66lgO2CHm"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_vocab(dataset_dict: DatasetDict) -> Dict:\n",
    "    \"\"\"Generate vocabulary from dataset transcriptions.\"\"\"\n",
    "    chars = set()\n",
    "    for split in [\"train\", \"validation\", \"test\"]:\n",
    "        for example in dataset_dict[split]:\n",
    "            chars.update(example[\"transcription\"].lower())  # Normalize to lowercase\n",
    "\n",
    "    # Add special tokens required by Wav2Vec2 CTC\n",
    "    vocab = [\"<pad>\", \"<unk>\", \"|\"] + sorted(chars)\n",
    "    return {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "# Generate and save vocabulary\n",
    "dataset = load_dataset(\"StefanStefan/STT\")\n",
    "vocab = generate_vocab(dataset)\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fine-Tuning script"
   ],
   "metadata": {
    "id": "NOYxj7cN_OEk"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8bueJpq0NqQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union, Optional\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, Features, Sequence, Value, load_dataset\n",
    "from transformers import (\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import jiwer\n",
    "import logging\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ConfigManager:\n",
    "    \"\"\"Manages configuration settings for the speech recognition pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        self.config = self._load_default_config()\n",
    "        if config_path:\n",
    "            self.update_config(config_path)\n",
    "\n",
    "    def _load_default_config(self) -> Dict:\n",
    "        return {\n",
    "            \"data\": {\n",
    "                \"dataset_name\": \"StefanStefan/STT\",\n",
    "                \"audio_dir\": \"1/output\"\n",
    "            },\n",
    "            \"processor\": {\n",
    "                \"pretrained_model_name_or_path\": \"facebook/wav2vec2-base-100h\"\n",
    "            },\n",
    "            \"training\": {\n",
    "                \"output_dir\": \"./wav2vec2-finetuned\",\n",
    "                \"group_by_length\": False,\n",
    "                \"lr_scheduler_type\": \"cosine_with_restarts\",\n",
    "                \"length_column_name\": \"length\",\n",
    "                \"per_device_train_batch_size\": 4,\n",
    "                \"per_device_eval_batch_size\": 8,\n",
    "                \"gradient_accumulation_steps\": 5,\n",
    "                \"load_best_model_at_end\": True,\n",
    "                \"metric_for_best_model\": \"wer\",\n",
    "                \"greater_is_better\": False,\n",
    "                \"eval_strategy\": \"steps\",\n",
    "                \"num_train_epochs\": 30,\n",
    "                \"fp16\": True,\n",
    "                \"save_steps\": 500,\n",
    "                \"eval_steps\": 500,\n",
    "                \"logging_steps\": 100,\n",
    "                \"learning_rate\": 4.48e-5,\n",
    "                \"weight_decay\": 0.01008,\n",
    "                \"warmup_ratio\": 0.1167,\n",
    "                \"max_grad_norm\": 0.3097,\n",
    "                \"save_total_limit\": 5,\n",
    "                \"save_strategy\": \"steps\",\n",
    "                \"report_to\": \"none\",\n",
    "                \"gradient_checkpointing\": True,\n",
    "                \"fp16_full_eval\": True,\n",
    "                \"dataloader_num_workers\": 4,\n",
    "                \"prediction_loss_only\": False,\n",
    "                \"optim\": \"adamw_torch\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def update_config(self, config_path: str) -> None:\n",
    "        \"\"\"Update configuration with values from a JSON file.\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            custom_config = json.load(f)\n",
    "        self.config = {**self.config, **custom_config}\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        \"\"\"Get the current configuration.\"\"\"\n",
    "        return self.config\n",
    "\n",
    "\n",
    "class LoggerSetup:\n",
    "    \"\"\"Sets up logging configuration.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def setup(level: int = logging.INFO) -> logging.Logger:\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "            level=level\n",
    "        )\n",
    "        return logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class DataProcessor(ABC):\n",
    "    \"\"\"Abstract base class for data processing.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AudioProcessor(DataProcessor):\n",
    "    \"\"\"Handles audio data processing.\"\"\"\n",
    "\n",
    "    def __init__(self, processor: Wav2Vec2Processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def process(self, audio_dict: dict) -> np.ndarray:\n",
    "        # Extract audio array and sampling rate from the dataset's audio dict\n",
    "        speech = audio_dict[\"array\"]\n",
    "        sampling_rate = audio_dict[\"sampling_rate\"]\n",
    "\n",
    "        # Resample if needed (using the processor's expected sampling rate)\n",
    "        if sampling_rate != self.processor.feature_extractor.sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                sampling_rate,\n",
    "                self.processor.feature_extractor.sampling_rate\n",
    "            )\n",
    "            speech = resampler(torch.tensor(speech)).squeeze().numpy()\n",
    "\n",
    "        return speech\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"Data collator that handles dynamic padding for CTC training.\"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "class DatasetManager:\n",
    "    \"\"\"Manages dataset operations.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict, processor: Wav2Vec2Processor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.audio_processor = AudioProcessor(processor)\n",
    "\n",
    "    def load_dataset_from_hf(self, dataset_name: str) -> DatasetDict:\n",
    "        \"\"\"Load dataset from Hugging Face.\"\"\"\n",
    "        dataset = load_dataset(dataset_name)\n",
    "        return dataset\n",
    "\n",
    "    def preprocess_function(self, batch: Dict) -> Dict:\n",
    "        \"\"\"Updated with text normalization.\"\"\"\n",
    "        # Normalize text to lowercase\n",
    "        batch[\"transcription\"] = batch[\"transcription\"].lower()\n",
    "\n",
    "        speech = self.audio_processor.process(batch[\"audio\"])\n",
    "\n",
    "        input_values = self.processor(\n",
    "            speech,\n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate,\n",
    "            return_attention_mask=False\n",
    "        ).input_values[0]\n",
    "\n",
    "        labels = self.processor(\n",
    "            text=batch[\"transcription\"],\n",
    "            return_attention_mask=False\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "    def create_dataset_dict(self) -> DatasetDict:\n",
    "        \"\"\"Create dataset dictionary from Hugging Face dataset.\"\"\"\n",
    "        dataset = self.load_dataset_from_hf(self.config[\"data\"][\"dataset_name\"])\n",
    "\n",
    "        # Assuming the dataset has 'train', 'validation', and 'test' splits\n",
    "        datasets = {\n",
    "            \"train\": dataset[\"train\"],\n",
    "            \"validation\": dataset[\"validation\"],\n",
    "            \"test\": dataset[\"test\"]\n",
    "        }\n",
    "\n",
    "        return DatasetDict(datasets)\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Manages model operations.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Dict, processor: Wav2Vec2Processor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "\n",
    "    def initialize_model(self) -> Wav2Vec2ForCTC:\n",
    "        \"\"\"Initialize and configure the model.\"\"\"\n",
    "        model = Wav2Vec2ForCTC.from_pretrained(\n",
    "            self.config[\"processor\"][\"pretrained_model_name_or_path\"],\n",
    "            vocab_size=len(self.processor.tokenizer),\n",
    "            pad_token_id=self.processor.tokenizer.pad_token_id,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "        model.config.label_pad_token_id = -100\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_metrics(pred, processor: Wav2Vec2Processor) -> Dict[str, float]:\n",
    "        \"\"\"Compute model metrics.\"\"\"\n",
    "        pred_logits = pred.predictions\n",
    "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "        pred_str = processor.batch_decode(pred_ids)\n",
    "\n",
    "        label_ids = pred.label_ids\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "        label_str = processor.batch_decode(label_ids, group_tokens=False)\n",
    "\n",
    "        return {\n",
    "            \"wer\": jiwer.wer(label_str, pred_str),\n",
    "            \"cer\": jiwer.cer(label_str, pred_str)\n",
    "        }\n",
    "\n",
    "\n",
    "class ProcessorManager:\n",
    "    def __init__(self, config: Dict):\n",
    "        \"\"\"\n",
    "        Initialize the ProcessorManager with configuration.\n",
    "\n",
    "        Args:\n",
    "            config (Dict): Configuration dictionary containing processor settings\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "\n",
    "    def initialize_processor(self) -> Wav2Vec2Processor:\n",
    "        \"\"\"Initialize processor with custom vocabulary.\"\"\"\n",
    "        # Load custom tokenizer\n",
    "        tokenizer = Wav2Vec2CTCTokenizer(\n",
    "            \"vocab.json\",\n",
    "            unk_token=\"<unk>\",\n",
    "            pad_token=\"<pad>\",\n",
    "            word_delimiter_token=\"|\"\n",
    "        )\n",
    "\n",
    "        # Load feature extractor from pretrained model\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "            self.config[\"processor\"][\"pretrained_model_name_or_path\"]\n",
    "        )\n",
    "\n",
    "        return Wav2Vec2Processor(\n",
    "            feature_extractor=feature_extractor,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "\n",
    "class TrainingPipeline:\n",
    "    def __init__(self, config_path: Optional[str] = None):\n",
    "        self.logger = LoggerSetup.setup()\n",
    "        self.config_manager = ConfigManager(config_path)\n",
    "        self.config = self.config_manager.get_config()\n",
    "\n",
    "        # First load dataset to generate vocab\n",
    "        temp_dataset = load_dataset(self.config[\"data\"][\"dataset_name\"])\n",
    "        if not os.path.exists(\"vocab.json\"):\n",
    "            self.logger.info(\"Generating vocabulary...\")\n",
    "            vocab = generate_vocab(temp_dataset)\n",
    "            with open(\"vocab.json\", \"w\") as f:\n",
    "                json.dump(vocab, f)\n",
    "\n",
    "        # Now initialize processor with custom vocab\n",
    "        self.processor_manager = ProcessorManager(self.config)\n",
    "        self.processor = self.processor_manager.initialize_processor()\n",
    "\n",
    "        # Rest of initialization remains the same\n",
    "        self.dataset_manager = DatasetManager(self.config, self.processor)\n",
    "        self.model_manager = ModelManager(self.config, self.processor)\n",
    "\n",
    "    def prepare_datasets(self) -> DatasetDict:\n",
    "        \"\"\"Prepare and preprocess datasets.\"\"\"\n",
    "        self.logger.info(\"Loading and preprocessing datasets...\")\n",
    "        dataset_dict = self.dataset_manager.create_dataset_dict()\n",
    "\n",
    "        # Get all original column names to remove\n",
    "        original_columns = dataset_dict[\"train\"].column_names\n",
    "\n",
    "        processed_dataset = dataset_dict.map(\n",
    "            self.dataset_manager.preprocess_function,\n",
    "            remove_columns=original_columns,\n",
    "            batched=False,\n",
    "            num_proc=4,\n",
    "            desc=\"Preprocessing the dataset\"\n",
    "        )\n",
    "\n",
    "        features = Features({\n",
    "            'input_values': Sequence(Value('float32')),\n",
    "            'labels': Sequence(Value('int64')),\n",
    "        })\n",
    "\n",
    "        return processed_dataset.cast(features)\n",
    "\n",
    "    def initialize_trainer(self, processed_dataset: DatasetDict) -> Trainer:\n",
    "        \"\"\"Initialize the trainer.\"\"\"\n",
    "        training_args = TrainingArguments(**self.config[\"training\"])\n",
    "\n",
    "        model = self.model_manager.initialize_model()\n",
    "        data_collator = DataCollatorCTCWithPadding(processor=self.processor)\n",
    "\n",
    "        return Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=processed_dataset[\"train\"],\n",
    "            eval_dataset=processed_dataset[\"validation\"],\n",
    "            compute_metrics=lambda pred: self.model_manager.compute_metrics(pred, self.processor)\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Execute the complete training pipeline.\"\"\"\n",
    "        try:\n",
    "            processed_dataset = self.prepare_datasets()\n",
    "            trainer = self.initialize_trainer(processed_dataset)\n",
    "\n",
    "            self.logger.info(\"Starting training...\")\n",
    "            trainer.train()\n",
    "\n",
    "            self.logger.info(\"Evaluating on test set...\")\n",
    "            test_results = trainer.evaluate(eval_dataset=processed_dataset[\"test\"])\n",
    "            self.logger.info(f\"Test Results: {test_results}\")\n",
    "\n",
    "            save_path = \"my-wav2vec2-finetuned\"\n",
    "            trainer.save_model(save_path)\n",
    "            self.processor.save_pretrained(save_path)\n",
    "            self.logger.info(f\"Model and processor saved to {save_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def main():\n",
    "    pipeline = TrainingPipeline()\n",
    "    pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quantization"
   ],
   "metadata": {
    "id": "ns0R2edy_Sj_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "U6wlxACva8eW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from datasets import load_dataset\n",
    "import torchaudio\n",
    "import jiwer\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def evaluate_transcription(model, processor, dataset, num_samples=10, device=\"cpu\"):\n",
    "    \"\"\"Evaluate a model on transcription quality.\"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Check if model is using half precision\n",
    "    is_half_precision = next(model.parameters()).dtype == torch.float16\n",
    "\n",
    "    # Select samples for evaluation\n",
    "    if num_samples and num_samples < len(dataset):\n",
    "        indices = np.linspace(0, len(dataset)-1, num_samples, dtype=int)\n",
    "        indices = [int(idx) for idx in indices]  # Convert to Python ints\n",
    "        test_dataset = dataset.select(indices)\n",
    "    else:\n",
    "        test_dataset = dataset\n",
    "\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx in range(len(test_dataset)):\n",
    "        # Process audio\n",
    "        sample = test_dataset[idx]\n",
    "        audio = sample[\"audio\"]\n",
    "        speech = audio[\"array\"]\n",
    "        sampling_rate = audio[\"sampling_rate\"]\n",
    "\n",
    "        # Resample if needed\n",
    "        if sampling_rate != processor.feature_extractor.sampling_rate:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                sampling_rate,\n",
    "                processor.feature_extractor.sampling_rate\n",
    "            )\n",
    "            speech = resampler(torch.tensor(speech)).squeeze().numpy()\n",
    "\n",
    "        # Preprocess\n",
    "        inputs = processor(\n",
    "            speech,\n",
    "            sampling_rate=processor.feature_extractor.sampling_rate,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Move to device\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Convert input to half precision if model is in half precision\n",
    "        if is_half_precision:\n",
    "            inputs.input_values = inputs.input_values.half()\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_values=inputs.input_values)\n",
    "\n",
    "        # Decode\n",
    "        logits = outputs.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "        # Store results\n",
    "        predictions.append(transcription)\n",
    "        references.append(sample[\"transcription\"].lower())\n",
    "\n",
    "    # Calculate metrics\n",
    "    wer = jiwer.wer(references, predictions)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"wer\": wer,\n",
    "        \"inference_time\": elapsed_time / len(test_dataset),\n",
    "        \"predictions\": predictions,\n",
    "        \"references\": references\n",
    "    }\n",
    "\n",
    "\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"Get the size of a PyTorch model in MB.\"\"\"\n",
    "    # Save model to a temporary file to get its size\n",
    "    tmp_path = \"tmp_model.pt\"\n",
    "    torch.save(model.state_dict(), tmp_path)\n",
    "    size_mb = os.path.getsize(tmp_path) / (1024 * 1024)\n",
    "    os.remove(tmp_path)  # Clean up\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def apply_fp16_quantization(model, device='cpu'):\n",
    "    \"\"\"\n",
    "    Apply FP16 quantization to a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        device: Device to use\n",
    "\n",
    "    Returns:\n",
    "        FP16 quantized model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Applying FP16 quantization to the model...\")\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "\n",
    "        # Convert model to float16\n",
    "        quantized_model = model.half().to(device)\n",
    "        print(\"Successfully converted model to FP16\")\n",
    "        return quantized_model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FP16 quantization failed: {e}\")\n",
    "        return model\n",
    "\n",
    "\n",
    "def quantize_model(model_id, dataset_name, output_dir, num_test_samples=5):\n",
    "    \"\"\"\n",
    "    Quantize a Hugging Face model to FP16 and evaluate it.\n",
    "\n",
    "    Args:\n",
    "        model_id: ID of the Hugging Face model\n",
    "        dataset_name: ID of the dataset for evaluation\n",
    "        output_dir: Directory to save the results\n",
    "        num_test_samples: Number of samples to use for evaluation\n",
    "\n",
    "    Returns:\n",
    "        Dict containing information about the quantized model\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a separate directory for evaluation results\n",
    "    eval_dir = os.path.join(output_dir, \"evaluation_results\")\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "\n",
    "    # Load dataset\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    test_dataset = dataset[\"test\"]\n",
    "\n",
    "    # Load model and processor\n",
    "    print(f\"Loading model: {model_id}\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "\n",
    "    # Determine device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Step 1: Original Model Evaluation\n",
    "    print(\"Evaluating original model...\")\n",
    "    original_results = evaluate_transcription(\n",
    "        model, processor, test_dataset, num_test_samples, device\n",
    "    )\n",
    "\n",
    "    # Get original model size\n",
    "    original_size_mb = get_model_size_mb(model)\n",
    "\n",
    "    # Step 2: Apply FP16 quantization\n",
    "    print(\"\\nApplying FP16 quantization...\")\n",
    "    quantized_model = apply_fp16_quantization(model, device)\n",
    "\n",
    "    # Get quantized model size\n",
    "    quantized_size_mb = get_model_size_mb(quantized_model)\n",
    "\n",
    "    # Step 3: Evaluate quantized model\n",
    "    print(\"\\nEvaluating FP16 quantized model...\")\n",
    "    quantized_results = evaluate_transcription(\n",
    "        quantized_model, processor, test_dataset, num_test_samples, device\n",
    "    )\n",
    "\n",
    "    # Save quantized model and processor to the output directory\n",
    "    print(f\"Saving FP16 quantized model and processor to {output_dir}...\")\n",
    "    quantized_model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    print(f\"Successfully saved model and processor to {output_dir}\")\n",
    "\n",
    "    # Calculate WER change\n",
    "    wer_change_pct = 100 * (quantized_results[\"wer\"] - original_results[\"wer\"]) / original_results[\"wer\"]\n",
    "\n",
    "    # Calculate size reduction\n",
    "    size_reduction_pct = 100 * (original_size_mb - quantized_size_mb) / original_size_mb\n",
    "\n",
    "    # Create model info dictionary\n",
    "    model_info = {\n",
    "        \"original\": {\n",
    "            \"wer\": original_results[\"wer\"],\n",
    "            \"inference_time\": original_results[\"inference_time\"],\n",
    "            \"size_mb\": original_size_mb\n",
    "        },\n",
    "        \"quantized\": {\n",
    "            \"path\": output_dir,\n",
    "            \"size_mb\": quantized_size_mb,\n",
    "            \"wer\": quantized_results[\"wer\"],\n",
    "            \"wer_change_pct\": wer_change_pct,\n",
    "            \"inference_time\": quantized_results[\"inference_time\"],\n",
    "            \"quantization_type\": \"float16\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save results in the evaluation directory\n",
    "    with open(os.path.join(eval_dir, \"quantization_results.json\"), \"w\") as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "\n",
    "    # Generate markdown report\n",
    "    report_path = os.path.join(eval_dir, \"quantization_report.md\")\n",
    "    with open(report_path, \"w\") as f:\n",
    "        f.write(\"# Wav2Vec2 Model FP16 Quantization Report\\n\\n\")\n",
    "\n",
    "        f.write(\"## Model Information\\n\\n\")\n",
    "        f.write(f\"- Original model: {model_id}\\n\")\n",
    "        f.write(f\"- Dataset: {dataset_name}\\n\")\n",
    "        f.write(f\"- Quantization: FP16\\n\\n\")\n",
    "\n",
    "        f.write(\"## Quantization Results\\n\\n\")\n",
    "        f.write(\"| Format | WER | Inference Time (s) | Size (MB) |\\n\")\n",
    "        f.write(\"|--------|-----|---------------------|----------|\\n\")\n",
    "        f.write(f\"| Original | {original_results['wer']:.4f} | {original_results['inference_time']:.4f} | {original_size_mb:.2f} |\\n\")\n",
    "        f.write(f\"| FP16 | {quantized_results['wer']:.4f} | {quantized_results['inference_time']:.4f} | {quantized_size_mb:.2f} |\\n\\n\")\n",
    "\n",
    "        f.write(f\"- Size reduction from quantization: {size_reduction_pct:.2f}%\\n\")\n",
    "\n",
    "        f.write(\"## Analysis\\n\\n\")\n",
    "        f.write(f\"- WER change: {wer_change_pct:.2f}%\\n\")\n",
    "\n",
    "        inference_time_change = 100 * (quantized_results['inference_time'] - original_results['inference_time']) / original_results['inference_time']\n",
    "        f.write(f\"- Inference time change: {inference_time_change:.2f}%\\n\\n\")\n",
    "\n",
    "        # Add some conclusions\n",
    "        f.write(\"### Conclusion\\n\\n\")\n",
    "        f.write(f\"The FP16 quantization \")\n",
    "\n",
    "        if abs(wer_change_pct) < 1.0:\n",
    "            f.write(f\"was successful with negligible impact on accuracy ({wer_change_pct:.2f}% WER change). \")\n",
    "        elif wer_change_pct > 0:\n",
    "            f.write(f\"resulted in a {wer_change_pct:.2f}% increase in Word Error Rate, \")\n",
    "            f.write(\"which may be acceptable depending on your use case and the benefits gained. \")\n",
    "        else:\n",
    "            f.write(f\"surprisingly improved accuracy by {abs(wer_change_pct):.2f}%. \")\n",
    "\n",
    "        if inference_time_change < 0:\n",
    "            f.write(f\"The model also shows a {abs(inference_time_change):.2f}% improvement in inference speed. \")\n",
    "        else:\n",
    "            f.write(f\"However, the model is {inference_time_change:.2f}% slower than the original model. \")\n",
    "\n",
    "        f.write(f\"The quantization reduced the model size by {size_reduction_pct:.2f}%, \")\n",
    "        if abs(wer_change_pct) < 5.0:\n",
    "            f.write(\"with a reasonable trade-off in accuracy.\")\n",
    "        elif wer_change_pct > 0:\n",
    "            f.write(f\"but at the cost of a {wer_change_pct:.2f}% increase in WER.\")\n",
    "        else:\n",
    "            f.write(f\"while actually improving accuracy by {abs(wer_change_pct):.2f}%.\")\n",
    "\n",
    "    # Print summary to console\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FP16 QUANTIZATION RESULTS SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Original WER: {original_results['wer']:.4f}\")\n",
    "    print(f\"FP16 quantized WER: {quantized_results['wer']:.4f} ({wer_change_pct:.2f}% change)\")\n",
    "    print(f\"Original model size: {original_size_mb:.2f} MB\")\n",
    "    print(f\"FP16 quantized model size: {quantized_size_mb:.2f} MB\")\n",
    "    print(f\"Size reduction: {size_reduction_pct:.2f}%\")\n",
    "    print(f\"Quantized model path: {output_dir}\")\n",
    "    print(f\"\\nDetailed report saved to {report_path}\")\n",
    "\n",
    "    return model_info\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration\n",
    "    model_id = \"StefanStefan/Wav2Vec-100-CSR\"\n",
    "    dataset_name = \"StefanStefan/STT\"\n",
    "    output_dir = \"wav2vec2-fp16\"\n",
    "    num_test_samples = 5000\n",
    "\n",
    "    quantize_model(model_id, dataset_name, output_dir, num_test_samples)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "id": "8cJBub_D7W1D"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowledge Distillation"
   ],
   "metadata": {
    "id": "XN2beTpj_V0y"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlP0sMdbUCaO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2Config,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from datasets import load_dataset\n",
    "import jiwer\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "from prettytable import PrettyTable\n",
    "import pandas as pd\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\", level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"Data collator for CTC inference.\"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            label_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"attention_mask\"].ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        # Ensure we're using float32 (not float64/double)\n",
    "        if batch[\"input_values\"].dtype == torch.float64:\n",
    "            batch[\"input_values\"] = batch[\"input_values\"].to(torch.float32)\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset, processor):\n",
    "    \"\"\"Preprocess the dataset for speech recognition.\"\"\"\n",
    "    def preprocess_function(batch):\n",
    "        # Normalize text\n",
    "        batch[\"transcription\"] = batch[\"transcription\"].lower()\n",
    "\n",
    "        # Process audio\n",
    "        speech = batch[\"audio\"][\"array\"]\n",
    "        sampling_rate = batch[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "        # Resample if needed\n",
    "        if sampling_rate != processor.feature_extractor.sampling_rate:\n",
    "            import torchaudio\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                sampling_rate,\n",
    "                processor.feature_extractor.sampling_rate\n",
    "            )\n",
    "            speech = resampler(torch.tensor(speech)).squeeze().numpy()\n",
    "\n",
    "        # Get input values and labels\n",
    "        input_values = processor(\n",
    "            speech,\n",
    "            sampling_rate=processor.feature_extractor.sampling_rate,\n",
    "            return_attention_mask=False\n",
    "        ).input_values[0]\n",
    "\n",
    "        # Ensure we're using float32 (not float64/double)\n",
    "        input_values = np.array(input_values, dtype=np.float32)\n",
    "\n",
    "        # Apply SpecAugment-like time masking for training data\n",
    "        if batch.get(\"split\", \"\") == \"train\":\n",
    "            input_values_tensor = torch.tensor(input_values)\n",
    "            seq_len = input_values_tensor.shape[0]\n",
    "\n",
    "            # Time masking (mask random segments of the audio)\n",
    "            for _ in range(2):  # Apply 2 time masks\n",
    "                mask_length = int(seq_len * 0.05)  # 5% of sequence length\n",
    "                if mask_length > 0:\n",
    "                    start = np.random.randint(0, seq_len - mask_length)\n",
    "                    input_values_tensor[start:start+mask_length] = 0.0\n",
    "\n",
    "            input_values = input_values_tensor.numpy()\n",
    "\n",
    "        labels = processor(\n",
    "            text=batch[\"transcription\"],\n",
    "            return_attention_mask=False\n",
    "        ).input_ids\n",
    "\n",
    "        return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "    # Process all splits\n",
    "    processed_dataset = {}\n",
    "    for split in dataset:\n",
    "        logger.info(f\"Processing {split} split...\")\n",
    "        # Use smaller subset for faster iterations\n",
    "        subset_size = len(dataset[split]) if split == \"train\" else min(100, len(dataset[split]))\n",
    "        dataset_subset = dataset[split].select(range(subset_size))\n",
    "        logger.info(f\"Using {subset_size} examples from {split} split\")\n",
    "\n",
    "        processed_dataset[split] = dataset_subset.map(\n",
    "            preprocess_function,\n",
    "            remove_columns=dataset_subset.column_names,\n",
    "            num_proc=4,\n",
    "            desc=f\"Processing {split} split\"\n",
    "        )\n",
    "        logger.info(f\"Finished processing {split} split: {len(processed_dataset[split])} examples\")\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "def evaluate_model(model, processor, test_dataset, device, max_eval_samples=200):\n",
    "    \"\"\"Evaluate model on test dataset with validation loss.\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Use a larger subset for evaluation for more accurate WER measurement\n",
    "    if len(test_dataset) > max_eval_samples:\n",
    "        logger.info(f\"Evaluating on {max_eval_samples} samples instead of {len(test_dataset)}\")\n",
    "        test_dataset = test_dataset.select(range(max_eval_samples))\n",
    "    else:\n",
    "        logger.info(f\"Evaluating on all {len(test_dataset)} samples\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "    dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    # Track validation loss\n",
    "    val_loss = 0\n",
    "    val_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            # Ensure all tensors are float32\n",
    "            input_values = batch[\"input_values\"].to(device, dtype=torch.float32)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_values=input_values, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "            predicted_ids = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            label_ids = labels.cpu().numpy()\n",
    "\n",
    "            # Decode predictions and references\n",
    "            predictions = processor.batch_decode(predicted_ids)\n",
    "\n",
    "            # Clean up label ids (-100 -> pad_token_id)\n",
    "            label_ids_cleaned = np.where(label_ids == -100, processor.tokenizer.pad_token_id, label_ids)\n",
    "            references = processor.batch_decode(label_ids_cleaned, group_tokens=False)\n",
    "\n",
    "            all_predictions.extend(predictions)\n",
    "            all_references.extend(references)\n",
    "\n",
    "    # Calculate metrics\n",
    "    wer = jiwer.wer(all_references, all_predictions)\n",
    "    cer = jiwer.cer(all_references, all_predictions)\n",
    "    avg_val_loss = val_loss / val_steps if val_steps > 0 else float('inf')\n",
    "\n",
    "    # Store more examples for analysis\n",
    "    examples = list(zip(all_references[:20], all_predictions[:20]))\n",
    "\n",
    "    # Log WER calculation details\n",
    "    logger.info(f\"WER calculated on {len(all_references)} test samples: {wer:.4f}\")\n",
    "\n",
    "    return {\"wer\": wer, \"cer\": cer, \"val_loss\": avg_val_loss,\n",
    "            \"examples\": examples,\n",
    "            \"num_samples\": len(all_references)}\n",
    "\n",
    "\n",
    "def calculate_model_size(model, as_mb=True):\n",
    "    \"\"\"Calculate model size in parameters or MB.\"\"\"\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    if as_mb:\n",
    "        # Estimate model size in MB (4 bytes per float32 parameter)\n",
    "        size_mb = (num_params * 4) / (1024 * 1024)\n",
    "        return size_mb\n",
    "    else:\n",
    "        return num_params\n",
    "\n",
    "\n",
    "def copy_matching_layers(student_model, teacher_model):\n",
    "    \"\"\"Copy matching layer weights from teacher to student.\"\"\"\n",
    "    # Create parameter name mappings\n",
    "    teacher_params = dict(teacher_model.named_parameters())\n",
    "    student_params = dict(student_model.named_parameters())\n",
    "\n",
    "    # Find matching parameters and copy weights\n",
    "    copied_params = 0\n",
    "    total_params = len(student_params)\n",
    "\n",
    "    for name, param in student_model.named_parameters():\n",
    "        if name in teacher_params:\n",
    "            # If shapes match exactly, copy directly\n",
    "            if param.shape == teacher_params[name].shape:\n",
    "                param.data.copy_(teacher_params[name].data)\n",
    "                copied_params += 1\n",
    "            # For layers that were reduced, try to initialize from corresponding teacher layers\n",
    "            elif \"layers\" in name:\n",
    "                # Extract layer indices\n",
    "                parts = name.split(\".\")\n",
    "                for i, part in enumerate(parts):\n",
    "                    if part == \"layers\" and i+1 < len(parts) and parts[i+1].isdigit():\n",
    "                        student_layer_idx = int(parts[i+1])\n",
    "                        # Map student layer to teacher layer\n",
    "                        teacher_layers = teacher_model.config.num_hidden_layers\n",
    "                        student_layers = student_model.config.num_hidden_layers\n",
    "\n",
    "                        # Find corresponding teacher layer using proportional mapping\n",
    "                        teacher_layer_idx = min(\n",
    "                            teacher_layers - 1,\n",
    "                            int(student_layer_idx * (teacher_layers / student_layers))\n",
    "                        )\n",
    "\n",
    "                        # Create teacher parameter name with mapped index\n",
    "                        teacher_name = name.replace(\n",
    "                            f\"layers.{student_layer_idx}\",\n",
    "                            f\"layers.{teacher_layer_idx}\"\n",
    "                        )\n",
    "\n",
    "                        if teacher_name in teacher_params:\n",
    "                            if param.shape == teacher_params[teacher_name].shape:\n",
    "                                param.data.copy_(teacher_params[teacher_name].data)\n",
    "                                copied_params += 1\n",
    "\n",
    "    logger.info(f\"Copied weights for {copied_params}/{total_params} parameters\")\n",
    "    return student_model\n",
    "\n",
    "\n",
    "class DistilledWav2Vec2(torch.nn.Module):\n",
    "    \"\"\"Single-stage distilled Wav2Vec2 model with feature-level distillation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher_model,\n",
    "        student_config,\n",
    "        temperature=2.0,  # Lower temperature for sharper distribution\n",
    "        alpha_ce=0.5,     # Balanced weight for CTC loss\n",
    "        alpha_kd=0.4,     # Increased weight for knowledge distillation\n",
    "        alpha_feat=0.1    # Same weight for feature distillation\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Print info about dimensions\n",
    "        logger.info(f\"Teacher hidden_size: {teacher_model.config.hidden_size}\")\n",
    "        logger.info(f\"Student hidden_size: {student_config.hidden_size}\")\n",
    "\n",
    "        # Create student model\n",
    "        self.student = Wav2Vec2ForCTC(student_config)\n",
    "\n",
    "        # Copy matching weights from teacher to student for better initialization\n",
    "        self.student = copy_matching_layers(self.student, teacher_model)\n",
    "\n",
    "        # Copy CTC head from teacher to student for better initialization\n",
    "        if self.student.lm_head.out_features == teacher_model.lm_head.out_features:\n",
    "            logger.info(\"Copying CTC head from teacher to student\")\n",
    "            self.student.lm_head.weight.data = teacher_model.lm_head.weight.data.clone()\n",
    "            if hasattr(self.student.lm_head, 'bias') and self.student.lm_head.bias is not None:\n",
    "                self.student.lm_head.bias.data = teacher_model.lm_head.bias.data.clone()\n",
    "\n",
    "        # Teacher model (frozen)\n",
    "        self.teacher = teacher_model\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.teacher.eval()\n",
    "\n",
    "        # Distillation hyperparameters\n",
    "        self.temperature = temperature\n",
    "        self.alpha_ce = alpha_ce\n",
    "        self.alpha_kd = alpha_kd\n",
    "        self.alpha_feat = alpha_feat\n",
    "\n",
    "        # Create feature adapters if dimensions don't match\n",
    "        self.feat_adapter = None\n",
    "        if teacher_model.config.hidden_size != student_config.hidden_size:\n",
    "            self.feat_adapter = torch.nn.Linear(\n",
    "                student_config.hidden_size,\n",
    "                teacher_model.config.hidden_size\n",
    "            )\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None, labels=None):\n",
    "        \"\"\"Forward pass with distillation during training.\"\"\"\n",
    "        # Ensure inputs are float32\n",
    "        if input_values.dtype != torch.float32:\n",
    "            input_values = input_values.to(torch.float32)\n",
    "\n",
    "        # Get student outputs - request hidden states only (no attentions)\n",
    "        student_outputs = self.student(\n",
    "            input_values=input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True,    # Request hidden states\n",
    "            output_attentions=False,      # Don't request attentions\n",
    "            return_dict=True,\n",
    "            labels=labels if not self.training else None  # Don't compute CTC loss here if training\n",
    "        )\n",
    "\n",
    "        # For inference, just return student outputs\n",
    "        if not self.training or labels is None:\n",
    "            # If not training but we requested hidden states, create proper output format\n",
    "            if hasattr(student_outputs, 'loss') and student_outputs.loss is None and labels is not None:\n",
    "                # Manually compute CTC loss if needed\n",
    "                log_probs = torch.nn.functional.log_softmax(student_outputs.logits, dim=-1)\n",
    "                input_lengths = torch.full(\n",
    "                    (input_values.shape[0],),\n",
    "                    log_probs.shape[1],\n",
    "                    dtype=torch.long,\n",
    "                    device=log_probs.device\n",
    "                )\n",
    "                target_lengths = torch.sum(labels != -100, dim=1)\n",
    "                labels_no_pad = labels.clone()\n",
    "                labels_no_pad[labels_no_pad == -100] = 0  # Replace padding with valid token\n",
    "                loss = torch.nn.functional.ctc_loss(\n",
    "                    log_probs.transpose(0, 1),\n",
    "                    labels_no_pad,\n",
    "                    input_lengths,\n",
    "                    target_lengths,\n",
    "                    blank=0,  # Assuming 0 is blank/pad token\n",
    "                    reduction='mean'\n",
    "                )\n",
    "                student_outputs.loss = loss\n",
    "\n",
    "            return student_outputs\n",
    "\n",
    "        # For training, compute multi-component distillation loss\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher(\n",
    "                input_values=input_values,\n",
    "                attention_mask=attention_mask,\n",
    "                output_hidden_states=True,  # Request teacher hidden states\n",
    "                output_attentions=False,    # Don't request teacher attentions\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "        # 1. Modified CTC loss with focal loss component\n",
    "        gamma = 2.0  # Focal loss gamma parameter\n",
    "        log_probs = torch.nn.functional.log_softmax(student_outputs.logits, dim=-1)\n",
    "        input_lengths = torch.full(\n",
    "            (input_values.shape[0],),\n",
    "            log_probs.shape[1],\n",
    "            dtype=torch.long,\n",
    "            device=log_probs.device\n",
    "        )\n",
    "        target_lengths = torch.sum(labels != -100, dim=1)\n",
    "        labels_no_pad = labels.clone()\n",
    "        labels_no_pad[labels_no_pad == -100] = 0  # Replace padding with valid token\n",
    "\n",
    "        # Handle cases where all labels might be padding\n",
    "        if torch.all(target_lengths == 0):\n",
    "            ctc_loss = torch.tensor(0.0, device=log_probs.device)\n",
    "        else:\n",
    "            # Standard CTC loss\n",
    "            standard_ctc_loss = torch.nn.functional.ctc_loss(\n",
    "                log_probs.transpose(0, 1),\n",
    "                labels_no_pad,\n",
    "                input_lengths,\n",
    "                target_lengths,\n",
    "                blank=0,  # Assuming 0 is blank/pad token\n",
    "                reduction='none'  # Get per-sample loss\n",
    "            )\n",
    "\n",
    "            # Apply focal loss modulation - focus more on hard examples\n",
    "            pt = torch.exp(-standard_ctc_loss)\n",
    "            focal_weight = (1 - pt) ** gamma\n",
    "\n",
    "            # Final focal CTC loss\n",
    "            ctc_loss = torch.mean(standard_ctc_loss * focal_weight)\n",
    "\n",
    "        # 2. CTC Logit Distillation using KL divergence\n",
    "        # First ensure the sequence lengths match by resampling if needed\n",
    "        teacher_logits = teacher_outputs.logits  # [batch, teacher_seq_len, vocab_size]\n",
    "        student_logits = student_outputs.logits  # [batch, student_seq_len, vocab_size]\n",
    "\n",
    "        # Check if sequence lengths match\n",
    "        if teacher_logits.size(1) != student_logits.size(1):\n",
    "            # Interpolate student logits to match teacher sequence length\n",
    "            # Reshape for interpolation [batch, seq_len, vocab] -> [batch, vocab, seq_len]\n",
    "            student_logits_trans = student_logits.transpose(1, 2)\n",
    "            # Interpolate to match teacher sequence length\n",
    "            student_logits_resized = torch.nn.functional.interpolate(\n",
    "                student_logits_trans,\n",
    "                size=teacher_logits.size(1),\n",
    "                mode='linear'\n",
    "            )\n",
    "            # Reshape back [batch, vocab, seq_len] -> [batch, seq_len, vocab]\n",
    "            student_logits = student_logits_resized.transpose(1, 2)\n",
    "\n",
    "        # Now apply temperature and KL divergence\n",
    "        soft_student_logits = torch.nn.functional.log_softmax(\n",
    "            student_logits / self.temperature, dim=-1\n",
    "        )\n",
    "        soft_teacher_logits = torch.nn.functional.softmax(\n",
    "            teacher_logits / self.temperature, dim=-1\n",
    "        )\n",
    "\n",
    "        kd_loss = torch.nn.functional.kl_div(\n",
    "            soft_student_logits,\n",
    "            soft_teacher_logits,\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "\n",
    "        # 3. Feature-level distillation with cosine similarity\n",
    "        feat_loss = 0.0\n",
    "        if self.alpha_feat > 0:\n",
    "            student_hidden_states = student_outputs.hidden_states\n",
    "            teacher_hidden_states = teacher_outputs.hidden_states\n",
    "\n",
    "            s_layers = len(student_hidden_states)\n",
    "            t_layers = len(teacher_hidden_states)\n",
    "\n",
    "            # Map all student layers to teacher layers using more sophisticated mapping\n",
    "            layer_pairs = []\n",
    "            for s_idx in range(s_layers):\n",
    "                # Map student layer index to corresponding teacher layer index\n",
    "                t_idx = min(t_layers - 1, int(s_idx * (t_layers / s_layers)))\n",
    "                layer_pairs.append((s_idx, t_idx))\n",
    "\n",
    "            # Compute cosine similarity loss on all matched layer pairs\n",
    "            total_feat_loss = 0.0\n",
    "            for s_idx, t_idx in layer_pairs:\n",
    "                s_feat = student_hidden_states[s_idx]\n",
    "                t_feat = teacher_hidden_states[t_idx]\n",
    "\n",
    "                # Handle sequence length differences with interpolation if needed\n",
    "                if s_feat.size(1) != t_feat.size(1):\n",
    "                    # Interpolate student features to match teacher sequence length\n",
    "                    s_feat_trans = s_feat.transpose(1, 2)\n",
    "                    s_feat_resized = torch.nn.functional.interpolate(\n",
    "                        s_feat_trans,\n",
    "                        size=t_feat.size(1),\n",
    "                        mode='linear'\n",
    "                    )\n",
    "                    s_feat = s_feat_resized.transpose(1, 2)\n",
    "\n",
    "                # Apply adapter if dimensions don't match\n",
    "                if self.feat_adapter is not None:\n",
    "                    s_feat = self.feat_adapter(s_feat)\n",
    "\n",
    "                # Compute cosine similarity instead of MSE (1 - cosine_similarity)\n",
    "                s_feat_norm = torch.nn.functional.normalize(s_feat, p=2, dim=-1)\n",
    "                t_feat_norm = torch.nn.functional.normalize(t_feat, p=2, dim=-1)\n",
    "\n",
    "                # Calculate cosine similarity loss (mean over sequence length)\n",
    "                similarity = torch.sum(s_feat_norm * t_feat_norm, dim=-1)\n",
    "                cos_loss = torch.mean(1.0 - similarity)\n",
    "\n",
    "                # Add weighted loss (give more weight to later layers)\n",
    "                layer_weight = 0.5 + 0.5 * (s_idx / (s_layers - 1))  # Weight increases with layer depth\n",
    "                total_feat_loss += cos_loss * layer_weight\n",
    "\n",
    "            # Normalize by total weight\n",
    "            feat_loss = total_feat_loss / sum([0.5 + 0.5 * (i / (s_layers - 1)) for i in range(s_layers)])\n",
    "\n",
    "        # Combine losses\n",
    "        combined_loss = (\n",
    "            self.alpha_ce * ctc_loss +\n",
    "            self.alpha_kd * kd_loss +\n",
    "            self.alpha_feat * feat_loss\n",
    "        )\n",
    "\n",
    "        # Return with updated loss\n",
    "        student_outputs.loss = combined_loss\n",
    "        return student_outputs\n",
    "\n",
    "\n",
    "def create_student_config(teacher_config, size_reduction=0.15):\n",
    "    \"\"\"Create a student configuration for distillation.\"\"\"\n",
    "    # Start with a copy of the teacher config\n",
    "    student_config = Wav2Vec2Config.from_dict(teacher_config.to_dict())\n",
    "\n",
    "    # Reduce number of layers more carefully based on importance\n",
    "    original_layers = teacher_config.num_hidden_layers\n",
    "    # Keep more layers (at least 70% of original)\n",
    "    target_layers = max(6, int(original_layers * (1 - size_reduction * 0.7)))\n",
    "    student_config.num_hidden_layers = target_layers\n",
    "\n",
    "    # Keep the original hidden size to avoid dimensionality issues\n",
    "    # Hidden size must be divisible by conv groups and attention heads\n",
    "    # Safer to maintain the original hidden size to avoid compatibility issues\n",
    "    hidden_size = teacher_config.hidden_size\n",
    "\n",
    "    # Reduce intermediate size less aggressively\n",
    "    student_config.intermediate_size = int(teacher_config.intermediate_size * 0.8)\n",
    "\n",
    "    # Ensure intermediate size is divisible by 8 for better hardware utilization\n",
    "    student_config.intermediate_size = (student_config.intermediate_size // 8) * 8\n",
    "\n",
    "    # Adjust attention heads - ensuring divisibility\n",
    "    if teacher_config.num_attention_heads > 8:\n",
    "        # Find all possible divisors of the hidden size\n",
    "        divisors = [i for i in range(1, min(teacher_config.num_attention_heads, 16) + 1)\n",
    "                  if student_config.hidden_size % i == 0]\n",
    "\n",
    "        # Choose the largest divisor that's smaller than the original head count\n",
    "        for divisor in sorted(divisors, reverse=True):\n",
    "            if divisor < teacher_config.num_attention_heads:\n",
    "                student_config.num_attention_heads = divisor\n",
    "                break\n",
    "\n",
    "    # Dropout adjustments for better convergence\n",
    "    student_config.hidden_dropout = 0.1\n",
    "    student_config.attention_dropout = 0.1\n",
    "    student_config.activation_dropout = 0.1\n",
    "    student_config.feat_proj_dropout = 0.0\n",
    "\n",
    "    # Log the configuration differences\n",
    "    logger.info(\"Student Model Configuration:\")\n",
    "    logger.info(f\"Teacher config: layers={original_layers}, hidden={teacher_config.hidden_size}, heads={teacher_config.num_attention_heads}\")\n",
    "    logger.info(f\"Student config: layers={target_layers}, hidden={student_config.hidden_size}, heads={student_config.num_attention_heads}\")\n",
    "\n",
    "    return student_config\n",
    "\n",
    "\n",
    "def train_distilled_model(\n",
    "    distilled_model,\n",
    "    processor,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    test_dataset,\n",
    "    device,\n",
    "    epochs=5,\n",
    "    batch_size=4,\n",
    "    learning_rate=3e-5,\n",
    "    eval_steps=500,\n",
    "    save_path=None,\n",
    "    gradient_accumulation_steps=2  # New parameter for gradient accumulation\n",
    "):\n",
    "    \"\"\"Train a distilled model with tabular output of metrics at each evaluation step.\"\"\"\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator\n",
    "    )\n",
    "\n",
    "    # Calculate total steps and prepare progress tracking\n",
    "    total_steps = len(train_dataloader) * epochs // gradient_accumulation_steps\n",
    "    step_count = 0\n",
    "    steps_since_eval = 0\n",
    "\n",
    "    # Setup optimizer with parameter groups and weight decay\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in distilled_model.student.named_parameters()\n",
    "                      if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.01,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in distilled_model.student.named_parameters()\n",
    "                      if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Add adapter parameters if they exist\n",
    "    if hasattr(distilled_model, 'feat_adapter') and distilled_model.feat_adapter is not None:\n",
    "        adapter_params = list(distilled_model.feat_adapter.parameters())\n",
    "        if adapter_params:\n",
    "            optimizer_grouped_parameters.append({\n",
    "                \"params\": adapter_params,\n",
    "                \"weight_decay\": 0.01,\n",
    "            })\n",
    "\n",
    "    # Create optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        eps=1e-6  # Slightly higher for better stability\n",
    "    )\n",
    "\n",
    "    # Create scheduler with one-cycle policy for better convergence\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.1,  # Warmup for 10% of training\n",
    "        div_factor=10.0,  # initial_lr = max_lr / div_factor\n",
    "        final_div_factor=100.0  # final_lr = initial_lr / final_div_factor\n",
    "    )\n",
    "\n",
    "    # Create results directory if saving\n",
    "    if save_path:\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        metrics_file = os.path.join(save_path, \"training_metrics.csv\")\n",
    "        metrics_md_file = os.path.join(save_path, \"training_metrics.md\")\n",
    "\n",
    "    # Setup metrics storage for tabular output\n",
    "    metrics_data = {\n",
    "        \"Step\": [],\n",
    "        \"Training Loss\": [],\n",
    "        \"Validation Loss\": [],\n",
    "        \"Wer\": [],\n",
    "        \"Cer\": []\n",
    "    }\n",
    "\n",
    "    # Training variables\n",
    "    distilled_model.to(device)\n",
    "    best_wer = float('inf')\n",
    "    best_model = None\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    logger.info(f\"Starting training for {epochs} epochs ({total_steps} steps)\")\n",
    "    logger.info(f\"Evaluating every {eval_steps} steps\")\n",
    "\n",
    "    try:\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            steps_in_epoch = len(train_dataloader)\n",
    "\n",
    "            # Training epoch\n",
    "            distilled_model.train()\n",
    "            progress_bar = tqdm(train_dataloader,\n",
    "                            desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "                            total=steps_in_epoch)\n",
    "\n",
    "            # Progressive distillation schedule - adjust at the start of each epoch\n",
    "            progress = epoch / epochs\n",
    "            distilled_model.alpha_kd = max(0.1, distilled_model.alpha_kd * (1.0 - 0.5 * progress))\n",
    "            distilled_model.alpha_feat = max(0.05, distilled_model.alpha_feat * (1.0 - 0.5 * progress))\n",
    "            distilled_model.alpha_ce = 1.0 - (distilled_model.alpha_kd + distilled_model.alpha_feat)\n",
    "\n",
    "            logger.info(f\"Epoch {epoch+1}: alpha_ce={distilled_model.alpha_ce:.3f}, \"\n",
    "                        f\"alpha_kd={distilled_model.alpha_kd:.3f}, \"\n",
    "                        f\"alpha_feat={distilled_model.alpha_feat:.3f}\")\n",
    "\n",
    "            # Learning rate warmup for first epoch\n",
    "            if epoch == 0:\n",
    "                for i, param_group in enumerate(optimizer.param_groups):\n",
    "                    param_group['lr'] = learning_rate * min(1.0, step_count / (0.1 * len(train_dataloader)))\n",
    "\n",
    "            optimizer.zero_grad()  # Zero gradients at the beginning\n",
    "\n",
    "            for i, batch in enumerate(progress_bar):\n",
    "                # Ensure tensors are float32\n",
    "                input_values = batch[\"input_values\"].to(device, dtype=torch.float32)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                try:\n",
    "                    outputs = distilled_model(input_values=input_values, labels=labels)\n",
    "                    # Scale the loss for gradient accumulation\n",
    "                    loss = outputs.loss / gradient_accumulation_steps\n",
    "\n",
    "                    # Layer-wise regularization for better generalization\n",
    "                    l2_reg = 0.0\n",
    "                    for name, param in distilled_model.student.named_parameters():\n",
    "                        if 'layers' in name and 'weight' in name:\n",
    "                            # Extract layer index more safely using regex\n",
    "                            import re\n",
    "                            match = re.search(r'layers\\.(\\d+)\\.', name)\n",
    "                            if match:\n",
    "                                layer_idx = int(match.group(1))\n",
    "                                # Apply stronger regularization to earlier layers\n",
    "                                layer_weight = 1.0 / (layer_idx + 1)  # Higher weight for earlier layers\n",
    "                                l2_reg += layer_weight * torch.sum(param ** 2)\n",
    "\n",
    "                    # Add small regularization loss\n",
    "                    if l2_reg > 0:\n",
    "                        reg_lambda = 1e-5  # Small weight for regularization\n",
    "                        loss = loss + reg_lambda * l2_reg\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    # Only update weights after accumulating enough gradients\n",
    "                    if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_dataloader):\n",
    "                        # Gradient clipping\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            list(distilled_model.student.parameters()) +\n",
    "                            (list(distilled_model.feat_adapter.parameters())\n",
    "                             if hasattr(distilled_model, 'feat_adapter') and\n",
    "                                distilled_model.feat_adapter is not None else []),\n",
    "                            max_norm=3.0\n",
    "                        )\n",
    "\n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        step_count += 1\n",
    "                        steps_since_eval += 1\n",
    "\n",
    "                    epoch_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "                    # Update progress bar\n",
    "                    progress_bar.set_postfix({\n",
    "                        'loss': f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n",
    "                        'lr': f\"{scheduler.get_last_lr()[0]:.6f}\"\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error in training step: {e}\")\n",
    "                    continue\n",
    "\n",
    "                # Regular evaluation\n",
    "                if steps_since_eval >= eval_steps:\n",
    "                    steps_since_eval = 0\n",
    "\n",
    "                    # Evaluation\n",
    "                    distilled_model.eval()\n",
    "                    student_model = distilled_model.student\n",
    "                    try:\n",
    "                        eval_metrics = evaluate_model(\n",
    "                            student_model,\n",
    "                            processor,\n",
    "                            eval_dataset,\n",
    "                            device,\n",
    "                            max_eval_samples=200 # Increased evaluation samples for better WER accuracy\n",
    "                        )\n",
    "\n",
    "                        # Store metrics for table\n",
    "                        metrics_data[\"Step\"].append(step_count)\n",
    "                        metrics_data[\"Training Loss\"].append(f\"{loss.item() * gradient_accumulation_steps:.6f}\")\n",
    "                        metrics_data[\"Validation Loss\"].append(f\"{eval_metrics['val_loss']:.6f}\")\n",
    "                        metrics_data[\"Wer\"].append(f\"{eval_metrics['wer']:.6f}\")\n",
    "                        metrics_data[\"Cer\"].append(f\"{eval_metrics['cer']:.6f}\")\n",
    "\n",
    "                        # Display only the latest metrics in a consistent table format\n",
    "                        if step_count == eval_steps:  # First evaluation\n",
    "                            # Create table header\n",
    "                            header = f\"{'Step':<8} {'Train Loss':<12} {'Valid Loss':<12} {'WER':<10} {'CER':<10}\"\n",
    "                            divider = \"-\" * len(header)\n",
    "                            print(\"\\n\" + divider)\n",
    "                            print(header)\n",
    "                            print(divider)\n",
    "\n",
    "                        # Print only the latest row\n",
    "                        latest = f\"{step_count:<8} {loss.item() * gradient_accumulation_steps:<12.6f} {eval_metrics['val_loss']:<12.6f} {eval_metrics['wer']:<10.6f} {eval_metrics['cer']:<10.6f}\"\n",
    "                        print(latest)\n",
    "\n",
    "                        # Print example predictions - more comprehensive\n",
    "                        print(f\"\\nExample predictions (from {eval_metrics['num_samples']} test samples):\")\n",
    "                        for i, (ref, pred) in enumerate(eval_metrics['examples'][:20]):\n",
    "                            print(f\"Example {i+1}:\")\n",
    "                            print(f\"Reference: {ref}\")\n",
    "                            print(f\"Prediction: {pred}\")\n",
    "                            print(\"-\" * 40)\n",
    "\n",
    "                        # Save best model\n",
    "                        if eval_metrics[\"wer\"] < best_wer:\n",
    "                            best_wer = eval_metrics[\"wer\"]\n",
    "                            best_model = student_model\n",
    "                            logger.info(f\"New best model with WER: {best_wer*100:.2f}%\")\n",
    "\n",
    "                            if save_path:\n",
    "                                best_model_path = os.path.join(save_path, \"best_model\")\n",
    "                                os.makedirs(best_model_path, exist_ok=True)\n",
    "                                best_model.save_pretrained(best_model_path)\n",
    "                                processor.save_pretrained(best_model_path)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error during evaluation: {e}\")\n",
    "\n",
    "                    # Return to training mode\n",
    "                    distilled_model.train()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training error: {e}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    logger.info(f\"Final evaluation on test set...\")\n",
    "    if best_model is None:\n",
    "        best_model = distilled_model.student\n",
    "\n",
    "    test_metrics = evaluate_model(best_model, processor, test_dataset, device, max_eval_samples=100)\n",
    "    logger.info(f\"Test WER: {test_metrics['wer']*100:.2f}%\")\n",
    "\n",
    "    # Save final model if best model wasn't saved\n",
    "    if save_path:\n",
    "        final_model_path = os.path.join(save_path, \"final_model\")\n",
    "        os.makedirs(final_model_path, exist_ok=True)\n",
    "        best_model.save_pretrained(final_model_path)\n",
    "        processor.save_pretrained(final_model_path)\n",
    "        logger.info(f\"Saved final model to {final_model_path}\")\n",
    "\n",
    "        # Generate full metrics table at the end of training\n",
    "        print(\"\\n\")\n",
    "        print(\"Full Training Metrics History:\")\n",
    "        # Display all collected metrics in a clear tabular format\n",
    "        headers = [\"Step\", \"Train Loss\", \"Valid Loss\", \"WER\", \"CER\"]\n",
    "        row_format = \"{:<8} {:<12} {:<12} {:<10} {:<10}\"\n",
    "\n",
    "        # Print headers\n",
    "        divider = \"-\" * 60\n",
    "        print(divider)\n",
    "        print(row_format.format(*headers))\n",
    "        print(divider)\n",
    "\n",
    "        # Print all rows\n",
    "        for i in range(len(metrics_data[\"Step\"])):\n",
    "            print(row_format.format(\n",
    "                metrics_data[\"Step\"][i],\n",
    "                float(metrics_data[\"Training Loss\"][i]),\n",
    "                float(metrics_data[\"Validation Loss\"][i]),\n",
    "                float(metrics_data[\"Wer\"][i]),\n",
    "                float(metrics_data[\"Cer\"][i])\n",
    "            ))\n",
    "        print(divider)\n",
    "\n",
    "    return best_model, metrics_data\n",
    "\n",
    "\n",
    "def distill_wav2vec2(\n",
    "    model_name,\n",
    "    dataset_name,\n",
    "    output_dir,\n",
    "    size_reduction=0.12, \n",
    "    temperature=2.0,      \n",
    "    epochs=15,           \n",
    "    batch_size=8,         \n",
    "    learning_rate=5e-5,   \n",
    "    eval_steps=500\n",
    "):\n",
    "    \"\"\"Single-stage distillation for Wav2Vec2 with tabular metrics output.\"\"\"\n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # Explicitly set default dtype to float32\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "    # 1. Load model, processor, and dataset\n",
    "    logger.info(f\"Loading model from {model_name}...\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "\n",
    "    logger.info(f\"Loading dataset from {dataset_name}...\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "\n",
    "    # 2. Preprocess dataset\n",
    "    logger.info(\"Preprocessing dataset...\")\n",
    "    processed_dataset = preprocess_dataset(dataset, processor)\n",
    "\n",
    "    # 3. Evaluate original model with increased sample size\n",
    "    logger.info(\"Evaluating original model...\")\n",
    "    original_metrics = evaluate_model(model, processor, processed_dataset[\"test\"], device, max_eval_samples=200)\n",
    "    logger.info(f\"Original model WER: {original_metrics['wer']*100:.2f}% (calculated on {original_metrics['num_samples']} samples)\")\n",
    "\n",
    "    original_size = calculate_model_size(model)\n",
    "    logger.info(f\"Original model size: {original_size:.2f} MB\")\n",
    "\n",
    "    # 4. Create student model for distillation\n",
    "    logger.info(\"Creating student model...\")\n",
    "    student_config = create_student_config(\n",
    "        model.config,\n",
    "        size_reduction=size_reduction\n",
    "    )\n",
    "\n",
    "    # Create distilled model with improved implementation\n",
    "    distillation_model = DistilledWav2Vec2(\n",
    "        teacher_model=model,\n",
    "        student_config=student_config,\n",
    "        temperature=temperature,\n",
    "        alpha_ce=0.5,      # Weight for CTC loss\n",
    "        alpha_kd=0.4,      # Weight for logit distillation\n",
    "        alpha_feat=0.1     # Weight for feature distillation\n",
    "    )\n",
    "\n",
    "    # 5. Train and get tabular metrics\n",
    "    logger.info(f\"Training distilled model for {epochs} epochs...\")\n",
    "    distilled_model, metrics_data = train_distilled_model(\n",
    "        distillation_model,\n",
    "        processor,\n",
    "        processed_dataset[\"train\"],\n",
    "        processed_dataset[\"validation\"],\n",
    "        processed_dataset[\"test\"],\n",
    "        device,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=learning_rate,\n",
    "        eval_steps=eval_steps,\n",
    "        save_path=output_dir,\n",
    "        gradient_accumulation_steps=2  # Enable gradient accumulation\n",
    "    )\n",
    "\n",
    "    # 6. Calculate size reduction\n",
    "    distilled_size = calculate_model_size(distilled_model)\n",
    "    size_reduction_pct = (original_size - distilled_size) / original_size * 100\n",
    "\n",
    "    # 7. Print final summary\n",
    "    logger.info(f\"Training completed!\")\n",
    "    logger.info(f\"Original model WER: {original_metrics['wer']*100:.2f}% (calculated on {original_metrics['num_samples']} samples)\")\n",
    "    logger.info(f\"Original model size: {original_size:.2f} MB\")\n",
    "\n",
    "    # Test final model with increased sample size for more accurate WER\n",
    "    final_metrics = evaluate_model(distilled_model, processor, processed_dataset[\"test\"], device, max_eval_samples=300)\n",
    "    logger.info(f\"Distilled model WER: {final_metrics['wer']*100:.2f}% (calculated on {final_metrics['num_samples']} samples)\")\n",
    "    logger.info(f\"Distilled model size: {distilled_size:.2f} MB\")\n",
    "    logger.info(f\"Size reduction: {size_reduction_pct:.2f}%\")\n",
    "    logger.info(f\"WER difference: {(final_metrics['wer'] - original_metrics['wer'])*100:.2f}%\")\n",
    "\n",
    "    # Create final metrics table as .md and .csv\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df.to_csv(os.path.join(output_dir, \"final_metrics.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(output_dir, \"final_metrics.md\"), 'w') as f:\n",
    "        f.write(\"# Knowledge Distillation Training Metrics\\n\\n\")\n",
    "        f.write(df.to_markdown(index=False))\n",
    "        f.write(\"\\n\\n## Summary\\n\\n\")\n",
    "        f.write(f\"- Original model size: {original_size:.2f} MB\\n\")\n",
    "        f.write(f\"- Distilled model size: {distilled_size:.2f} MB\\n\")\n",
    "        f.write(f\"- Size reduction: {size_reduction_pct:.2f}%\\n\")\n",
    "        f.write(f\"- Original WER: {original_metrics['wer']*100:.2f}% (calculated on {original_metrics['num_samples']} samples)\\n\")\n",
    "        f.write(f\"- Final WER: {final_metrics['wer']*100:.2f}% (calculated on {final_metrics['num_samples']} samples)\\n\")\n",
    "        f.write(f\"- WER change: {(final_metrics['wer'] - original_metrics['wer'])*100:.2f}%\\n\")\n",
    "\n",
    "        # Add example transcriptions for analysis\n",
    "        f.write(\"\\n\\n## Example Transcriptions\\n\\n\")\n",
    "        for i, (ref, pred) in enumerate(final_metrics['examples'][:20]):\n",
    "            f.write(f\"### Example {i+1}:\\n\")\n",
    "            f.write(f\"- **Reference**: {ref}\\n\")\n",
    "            f.write(f\"- **Prediction**: {pred}\\n\\n\")\n",
    "\n",
    "    return distilled_model, processor, metrics_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Single-stage distillation with tabular metrics output\n",
    "    distill_wav2vec2(\n",
    "        model_name=\"StefanStefan/Wav2Vec-100-CSR\",\n",
    "        dataset_name=\"StefanStefan/STT\",\n",
    "        output_dir=\"./wav2vec2_distillation\",\n",
    "        size_reduction=0.10,  # Reduced from 0.15 to keep more capacity\n",
    "        temperature=2.0,      # Lower temperature for better knowledge transfer\n",
    "        epochs=15,            # More epochs for better convergence\n",
    "        batch_size=8,         # Larger batch size with accumulation\n",
    "        learning_rate=5e-5,   # Slightly higher learning rate with one-cycle policy\n",
    "        eval_steps=500        # Evaluate every 500 steps to match table formatting\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model Size Calculator"
   ],
   "metadata": {
    "id": "azCD4UvW_bEG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87VqJ_jd-Qlm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "def get_directory_size(path):\n",
    "    \"\"\"Calculate the total size of a directory in MB.\"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "    return total_size / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def count_non_zero_parameters(model):\n",
    "    \"\"\"Count the number of non-zero parameters in a model.\"\"\"\n",
    "    return sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
    "\n",
    "def get_model_sparsity(model):\n",
    "    \"\"\"Calculate the sparsity of a model (percentage of zero parameters).\"\"\"\n",
    "    total_params = count_parameters(model)\n",
    "    non_zero_params = count_non_zero_parameters(model)\n",
    "    return (1 - non_zero_params / total_params) * 100\n",
    "\n",
    "def analyze_model(model_path, device='cpu'):\n",
    "    \"\"\"Analyze a model and return size statistics.\"\"\"\n",
    "    print(f\"Analyzing model at {model_path}...\")\n",
    "\n",
    "    # Measure disk usage\n",
    "    disk_size = get_directory_size(model_path)\n",
    "\n",
    "    # Load model and measure memory\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_path)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_path).to(device)\n",
    "\n",
    "    # Count parameters\n",
    "    total_params = count_parameters(model)\n",
    "    non_zero_params = count_non_zero_parameters(model)\n",
    "    sparsity = get_model_sparsity(model)\n",
    "\n",
    "    # Get file sizes for individual components\n",
    "    model_files = {}\n",
    "    for file in os.listdir(model_path):\n",
    "        file_path = os.path.join(model_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # MB\n",
    "            model_files[file] = file_size\n",
    "\n",
    "    return {\n",
    "        \"disk_size\": disk_size,\n",
    "        \"total_params\": total_params,\n",
    "        \"non_zero_params\": non_zero_params,\n",
    "        \"sparsity\": sparsity,\n",
    "        \"files\": model_files\n",
    "    }\n",
    "\n",
    "def format_number(num):\n",
    "    \"\"\"Format large numbers with commas.\"\"\"\n",
    "    return f\"{num:,.2f}\" if isinstance(num, float) else f\"{num:,}\"\n",
    "\n",
    "def print_model_comparison(original_stats, optimized_stats):\n",
    "    \"\"\"Print a comparison table between original and optimized models.\"\"\"\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Metric\", \"Original Model\", \"Optimized Model\", \"Difference\", \"Reduction %\"]\n",
    "\n",
    "    # Add disk size\n",
    "    disk_diff = original_stats[\"disk_size\"] - optimized_stats[\"disk_size\"]\n",
    "    disk_percent = (disk_diff / original_stats[\"disk_size\"]) * 100 if original_stats[\"disk_size\"] > 0 else 0\n",
    "    table.add_row([\n",
    "        \"Disk Size (MB)\",\n",
    "        f\"{original_stats['disk_size']:.2f}\",\n",
    "        f\"{optimized_stats['disk_size']:.2f}\",\n",
    "        f\"{disk_diff:.2f}\",\n",
    "        f\"{disk_percent:.2f}%\"\n",
    "    ])\n",
    "\n",
    "    # Add parameter counts\n",
    "    param_diff = original_stats[\"total_params\"] - optimized_stats[\"total_params\"]\n",
    "    param_percent = (param_diff / original_stats[\"total_params\"]) * 100 if original_stats[\"total_params\"] > 0 else 0\n",
    "    table.add_row([\n",
    "        \"Total Parameters\",\n",
    "        format_number(original_stats[\"total_params\"]),\n",
    "        format_number(optimized_stats[\"total_params\"]),\n",
    "        format_number(param_diff),\n",
    "        f\"{param_percent:.2f}%\"\n",
    "    ])\n",
    "\n",
    "    # Add non-zero parameter counts\n",
    "    nonzero_diff = original_stats[\"non_zero_params\"] - optimized_stats[\"non_zero_params\"]\n",
    "    nonzero_percent = (nonzero_diff / original_stats[\"non_zero_params\"]) * 100 if original_stats[\"non_zero_params\"] > 0 else 0\n",
    "    table.add_row([\n",
    "        \"Non-Zero Parameters\",\n",
    "        format_number(original_stats[\"non_zero_params\"]),\n",
    "        format_number(optimized_stats[\"non_zero_params\"]),\n",
    "        format_number(nonzero_diff),\n",
    "        f\"{nonzero_percent:.2f}%\"\n",
    "    ])\n",
    "\n",
    "    # Add sparsity\n",
    "    sparsity_diff = optimized_stats[\"sparsity\"] - original_stats[\"sparsity\"]\n",
    "    table.add_row([\n",
    "        \"Sparsity (%)\",\n",
    "        f\"{original_stats['sparsity']:.2f}%\",\n",
    "        f\"{optimized_stats['sparsity']:.2f}%\",\n",
    "        f\"{sparsity_diff:.2f}%\",\n",
    "        \"N/A\"\n",
    "    ])\n",
    "\n",
    "    print(table)\n",
    "\n",
    "    # Print file breakdown\n",
    "    print(\"\\nFile Size Breakdown (MB):\")\n",
    "    file_table = PrettyTable()\n",
    "    file_table.field_names = [\"File\", \"Original Size\", \"Optimized Size\", \"Difference\", \"Reduction %\"]\n",
    "\n",
    "    all_files = set(list(original_stats[\"files\"].keys()) + list(optimized_stats[\"files\"].keys()))\n",
    "    for file in sorted(all_files):\n",
    "        orig_size = original_stats[\"files\"].get(file, 0)\n",
    "        opt_size = optimized_stats[\"files\"].get(file, 0)\n",
    "        diff = orig_size - opt_size\n",
    "        percent = (diff / orig_size) * 100 if orig_size > 0 else 0\n",
    "\n",
    "        file_table.add_row([\n",
    "            file,\n",
    "            f\"{orig_size:.2f}\",\n",
    "            f\"{opt_size:.2f}\",\n",
    "            f\"{diff:.2f}\",\n",
    "            f\"{percent:.2f}%\"\n",
    "        ])\n",
    "\n",
    "    print(file_table)\n",
    "\n",
    "def main():\n",
    "\n",
    "    original_model_path = \"/content/wav2vec2-quantization_fp16_kd\"\n",
    "    optimized_model_path = \"/content/wav2vec2-quantization_fp16\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Analyze both models\n",
    "    original_stats = analyze_model(original_model_path, device)\n",
    "    optimized_stats = analyze_model(optimized_model_path, device)\n",
    "\n",
    "    # Print comparison\n",
    "    print(\"\\n=== Model Size Comparison ===\")\n",
    "    print_model_comparison(original_stats, optimized_stats)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xo3YrL0sELdP"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Push model to Hugging Face"
   ],
   "metadata": {
    "id": "CoIK4eiH_jKm"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m-QbPkTpTF0D"
   },
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "# Load your model and processor from the local folder\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"/content/wav2vec2-quantization_fp16\")\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"/content/wav2vec2-quantization_fp16\")\n",
    "\n",
    "# Push to the Hub (replace with your HF Hub repository name)\n",
    "repo_name = \"StefanStefan/Wav2Vec-100-CSR-Quantized\"\n",
    "model.push_to_hub(repo_name)\n",
    "processor.push_to_hub(repo_name)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
